{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_24_Cross-Validation-AND-Feature-Selection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "3ctjQBseh0Cw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_Lambda School Data Science - Model Validation_\n",
        "\n",
        "## Example solution to the Cross-Validation assignment — plus Feature Selection!\n",
        "\n",
        "See also Sebastian Raschka's example, [Basic Pipeline and Grid Search Setup](https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/svm_iris_pipeline_and_gridsearch.ipynb)."
      ]
    },
    {
      "metadata": {
        "id": "aHUVU_AMBKs0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We'll modify a project from Python Data Science Handbook by Jake VanderPlas\n",
        "# https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic\n",
        "    \n",
        "# Predicting Bicycle Traffic\n",
        "\n",
        "# As an example, let's take a look at whether we can predict the number of \n",
        "# bicycle trips across Seattle's Fremont Bridge based on weather, season, \n",
        "# and other factors.\n",
        "\n",
        "# We will join the bike data with another dataset, and try to determine the \n",
        "# extent to which weather and seasonal factors—temperature, precipitation, \n",
        "# and daylight hours—affect the volume of bicycle traffic through this corridor. \n",
        "# Fortunately, the NOAA makes available their daily weather station data \n",
        "# (I used station ID USW00024233) and we can easily use Pandas to join \n",
        "# the two data sources.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "\n",
        "def load(): \n",
        "    fremont_bridge = 'https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD'\n",
        "    \n",
        "    bicycle_weather = 'https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/data/BicycleWeather.csv'\n",
        "\n",
        "    counts = pd.read_csv(fremont_bridge, index_col='Date', parse_dates=True, \n",
        "                         infer_datetime_format=True)\n",
        "\n",
        "    weather = pd.read_csv(bicycle_weather, index_col='DATE', parse_dates=True, \n",
        "                          infer_datetime_format=True)\n",
        "\n",
        "    daily = counts.resample('d').sum()\n",
        "    daily['Total'] = daily.sum(axis=1)\n",
        "    daily = daily[['Total']] # remove other columns\n",
        "\n",
        "    weather_columns = ['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'AWND']\n",
        "    daily = daily.join(weather[weather_columns], how='inner')\n",
        "    \n",
        "    # Make a feature for yesterday's total\n",
        "    daily['Total_yesterday'] = daily.Total.shift(1)\n",
        "    daily = daily.drop(index=daily.index[0])\n",
        "    \n",
        "    return daily\n",
        "\n",
        "    \n",
        "def split(daily):\n",
        "    # Hold out an \"out-of-time\" test set, from the last 100 days of data\n",
        "    \n",
        "    train = daily[:-100]\n",
        "    test = daily[-100:]\n",
        "    \n",
        "    X_train = train.drop(columns='Total')\n",
        "    y_train = train.Total\n",
        "\n",
        "    X_test  = test.drop(columns='Total')\n",
        "    y_test  = test.Total\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def jake_wrangle(X):  \n",
        "    X = X.copy()\n",
        "\n",
        "    # patterns of use generally vary from day to day; \n",
        "    # let's add binary columns that indicate the day of the week:\n",
        "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "    for i, day in enumerate(days):\n",
        "        X[day] = (X.index.dayofweek == i).astype(float)\n",
        "\n",
        "\n",
        "    # we might expect riders to behave differently on holidays; \n",
        "    # let's add an indicator of this as well:\n",
        "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "    cal = USFederalHolidayCalendar()\n",
        "    holidays = cal.holidays('2012', '2016')\n",
        "    X = X.join(pd.Series(1, index=holidays, name='holiday'))\n",
        "    X['holiday'].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "    # We also might suspect that the hours of daylight would affect \n",
        "    # how many people ride; let's use the standard astronomical calculation \n",
        "    # to add this information:\n",
        "    def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
        "        \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
        "        days = (date - pd.datetime(2000, 12, 21)).days\n",
        "        m = (1. - np.tan(np.radians(latitude))\n",
        "             * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
        "        return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
        "\n",
        "    X['daylight_hrs'] = list(map(hours_of_daylight, X.index))\n",
        "\n",
        "\n",
        "    # temperatures are in 1/10 deg C; convert to C\n",
        "    X['TMIN'] /= 10\n",
        "    X['TMAX'] /= 10\n",
        "\n",
        "    # We can also calcuate the average temperature.\n",
        "    X['Temp (C)'] = 0.5 * (X['TMIN'] + X['TMAX'])\n",
        "\n",
        "\n",
        "    # precip is in 1/10 mm; convert to inches\n",
        "    X['PRCP'] /= 254\n",
        "\n",
        "    # In addition to the inches of precipitation, let's add a flag that \n",
        "    # indicates whether a day is dry (has zero precipitation):\n",
        "    X['dry day'] = (X['PRCP'] == 0).astype(int)\n",
        "\n",
        "\n",
        "    # Let's add a counter that increases from day 1, and measures how many \n",
        "    # years have passed. This will let us measure any observed annual increase \n",
        "    # or decrease in daily crossings:\n",
        "    X['annual'] = (X.index - X.index[0]).days / 365.\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def wrangle(X):\n",
        "    # From Daniel H (DS1 KotH)\n",
        "    X = X.copy()\n",
        "    X = X.replace(-9999, 0)\n",
        "    X = jake_wrangle(X)\n",
        "    \n",
        "    X['PRCP_yest'] = X.PRCP.shift(1).fillna(X.PRCP.mean())\n",
        "    X['Windchill'] = (((X['Temp (C)'] * (9/5) + 32) * .6215) + 34.74) - (35.75 * (X['AWND']** .16)) + (.4275 * (X['Temp (C)'])) * (X['AWND'] ** .16)\n",
        "    X['Rl_Cold'] = (((X['Temp (C)'] * (9/5) + 32) - X['Windchill']) -32) * (5/9)\n",
        "    X['TMIN_ln'] = X['TMIN'] **2\n",
        "    \n",
        "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "    for i, month in enumerate(months):\n",
        "        X[month] = (X.index.month == i+1).astype(float)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQN2O9PCAkBn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download and join data into a dataframe\n",
        "data = load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJrNph8IFyOM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = split(data)\n",
        "\n",
        "# Do the same wrangling to X_train and X_test\n",
        "X_train = wrangle(X_train)\n",
        "X_test  = wrangle(X_test)\n",
        "\n",
        "# Define an estimator and param_grid\n",
        "pipe = make_pipeline(\n",
        "    RobustScaler(), \n",
        "    SelectKBest(f_regression), \n",
        "    Ridge())\n",
        "\n",
        "param_grid = {\n",
        "    'selectkbest__k': range(1, len(X_train.columns)), \n",
        "    'ridge__alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AR4bo95ZJFwj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predict with X_test features\n",
        "y_pred = gs.predict(X_test)\n",
        "\n",
        "# Compare predictions to y_test labels\n",
        "test_score = mean_absolute_error(y_test, y_pred)\n",
        "print('Test Score:', test_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7MQYtnmxV159",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Or use the grid search's score method, \n",
        "# which combines these steps\n",
        "test_score = gs.score(X_test, y_test)\n",
        "print('Test Score:', -test_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qeb9ed8bIAE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Which features were selected?\n",
        "selector = gs.best_estimator_.named_steps['selectkbest']\n",
        "all_names = X_train.columns\n",
        "selected_mask = selector.get_support()\n",
        "selected_names = all_names[selected_mask]\n",
        "unselected_names = all_names[~selected_mask]\n",
        "\n",
        "print('Features selected:')\n",
        "for name in selected_names:\n",
        "    print(name)\n",
        "\n",
        "print()\n",
        "print('Features not selected:')\n",
        "for name in unselected_names:\n",
        "    print(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "urdgBk_Vp6ZN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BONUS: Recursive Feature Elimination!\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html"
      ]
    },
    {
      "metadata": {
        "id": "EEvISfuimpQV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "X_train_scaled = RobustScaler().fit_transform(X_train)\n",
        "rfe = RFECV(Ridge(alpha=1.0), scoring='neg_mean_absolute_error', cv=3)\n",
        "X_train_subset = rfe.fit_transform(X_train_scaled, y_train)\n",
        "\n",
        "all_names = X_train.columns\n",
        "selected_mask = rfe.support_\n",
        "selected_names = all_names[selected_mask]\n",
        "unselected_names = all_names[~selected_mask]\n",
        "\n",
        "print('Features selected:')\n",
        "for name in selected_names:\n",
        "    print(name)\n",
        "\n",
        "print()\n",
        "print('Features not selected:')\n",
        "for name in unselected_names:\n",
        "    print(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hoCUHkuGpjvX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train_subset = pd.DataFrame(X_train_subset, columns=selected_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZKoH475ppag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test_subset = rfe.transform(X_test)\n",
        "X_test_subset = pd.DataFrame(X_test_subset, columns=selected_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhcS0n8ApuUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_train_subset.shape, X_test.shape, X_test_subset.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OpHu92Yto88",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RFE again, but with polynomial features and interaction terms!"
      ]
    },
    {
      "metadata": {
        "id": "ArgRECEUtyH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_train_polynomial = poly.fit_transform(X_train)\n",
        "\n",
        "print(X_train.shape, X_train_polynomial.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TWVHh35wsdE7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_polynomial)\n",
        "\n",
        "rfe = RFECV(Ridge(alpha=1.0), scoring='neg_mean_absolute_error', \n",
        "            step=10, cv=3, verbose=1)\n",
        "\n",
        "X_train_subset = rfe.fit_transform(X_train_scaled, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TrMRGvezuDD7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_names = poly.get_feature_names(X_train.columns)\n",
        "selected_mask = rfe.support_\n",
        "selected_names = [name for name, selected in zip(all_names, selected_mask) if selected]\n",
        "\n",
        "print(f'{rfe.n_features_} Features selected:')\n",
        "for name in selected_names:\n",
        "    print(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pku8KixHv3CF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define an estimator and param_grid\n",
        "\n",
        "ridge = Ridge()\n",
        "\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 1.0, 10.]\n",
        "}\n",
        "\n",
        "# Fit on the train set, with grid search cross-validation\n",
        "gs = GridSearchCV(ridge, param_grid=param_grid, cv=3, \n",
        "                  scoring='neg_mean_absolute_error', \n",
        "                  verbose=1)\n",
        "\n",
        "gs.fit(X_train_subset, y_train)\n",
        "validation_score = gs.best_score_\n",
        "print()\n",
        "print('Cross-Validation Score:', -validation_score)\n",
        "print()\n",
        "print('Best estimator:', gs.best_estimator_)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnpbyDLSwPIH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Do the same transformations to X_test\n",
        "X_test_polynomial = poly.transform(X_test)\n",
        "X_test_scaled = scaler.transform(X_test_polynomial)\n",
        "X_test_subset = rfe.transform(X_test_scaled)\n",
        "\n",
        "# Use the grid search's score method with X_test_subset\n",
        "test_score = gs.score(X_test_subset, y_test)\n",
        "print('Test Score:', -test_score)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}